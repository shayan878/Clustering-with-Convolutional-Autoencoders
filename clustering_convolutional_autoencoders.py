# -*- coding: utf-8 -*-
"""NNDL_HWE_Q1_Sent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F56mjUjbuzBJWTQa4JIbvI_swwNYEHvS
"""

from random import randrange, randint
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms
import torchvision.models as models
from torch.optim import SGD, Adam, Adam
from torch.optim import lr_scheduler

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

num_epochs = 10
batch_size = 256
learning_rate = 1e-3 #lr = 0.001

# MNIST dataset
mnist_train = datasets.MNIST(root='./root', train=True, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize((0.1307,), (0.3081,))
                           ]))
mnist_test = datasets.MNIST(root='./root', train=False, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize((0.1307,), (0.3081,))
                           ]))
# Fashion-MNIST dataset
fashionmnist_train = datasets.FashionMNIST(root='./root', train=True, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize((0.2860,), (0.3530,))
                           ]))
fashionmnist_test = datasets.FashionMNIST(root='./root', train=False, download=True, transform=transforms.Compose([
                               transforms.ToTensor(),
                               transforms.Normalize((0.2860,), (0.3530,))
                           ]))

print('MNIST train shape:', len(mnist_train))
print('MNIST test shape:', len(mnist_test))
print('Fashion MNIST train shape:', len(fashionmnist_train))
print('Fashion MNIST test shape:', len(fashionmnist_test))

plt.figure(figsize=(10, 5))
plt.hist(mnist_train.targets, bins=range(11), align='left', rwidth=0.8, color='skyblue', edgecolor='black', alpha=0.7)
plt.xticks(range(10))
plt.xlabel('Labels')
plt.ylabel('Count')
plt.title('Label Distribution in MNIST Dataset')
plt.show()

plt.figure(figsize=(10, 5))
plt.hist(fashionmnist_train.targets, bins=range(11), align='left', rwidth=0.8, color='salmon', edgecolor='black', alpha=0.7)
plt.xticks(range(10))
plt.xlabel('Labels')
plt.ylabel('Count')
plt.title('Label Distribution in Fashion-MNIST Dataset')
plt.show()

######### MNIST ################
train_size_mnist = int(0.75 * len(mnist_train))
train_mnist, val_mnist = torch.utils.data.random_split(mnist_train, [train_size_mnist, len(mnist_train)-train_size_mnist])

train_mnist_loader = DataLoader(train_mnist, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
val_mnist_loader = DataLoader(val_mnist, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
test_mnist_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

######### Fashion-MNIST ################
train_size_fashionmnist = int(0.75 * len(fashionmnist_train))
train_fashionmnist, val_fashionmnist = torch.utils.data.random_split(fashionmnist_train, [train_size_mnist, len(fashionmnist_train)-train_size_fashionmnist])

train_fashionmnist_loader = DataLoader(train_fashionmnist, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
val_fashionmnist_loader = DataLoader(val_fashionmnist, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
test_fashionmnist_loader = DataLoader(fashionmnist_test, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

print(len(train_mnist))
print(len(val_mnist))
print(len(mnist_test))

print(len(train_fashionmnist))
print(len(val_fashionmnist))
print(len(fashionmnist_test))

def plot_dataset(data_loader):
    images, labels = next(iter(data_loader))

    plt.figure(figsize=(10, 10))
    for i in range(10):
        plt.subplot(5, 5, i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(1 - images[i].numpy().squeeze(), cmap="binary")
        plt.gca().set_facecolor('white')
    plt.show()

plot_dataset(train_mnist_loader)

plot_dataset(test_mnist_loader)

plot_dataset(train_fashionmnist_loader)

plot_dataset(test_fashionmnist_loader)

class ConvAutoencoder(nn.Module):
    def __init__(self):
        super(ConvAutoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, (3,3), stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d((2,2), stride=2, padding=0),
            nn.Conv2d(16, 8, (3,3), stride=1, padding=1),
            nn.ReLU(True),
            nn.MaxPool2d((2,2), stride=2, padding=0),
            nn.Conv2d(8, 8, (3,3), stride=1, padding=1),
            nn.MaxPool2d((2,2), stride=2, padding=1),
        )
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(8, 16, (3,3), stride=2, padding=2),
            nn.Conv2d(16, 16, (3,3), stride=1, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 8, (5,5), stride=3, padding=1),
            nn.Conv2d(8, 8, (3,3), stride=1, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(8, 1, (2,2), stride=2, padding=1),
            nn.Sigmoid()
        )
    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

model = ConvAutoencoder()
print(model)
encoder = nn.Sequential(*list(model.children())[:2])  # Extract encoder
print(encoder)

def train_eval_model(model, device, train_loader, val_loader, num_epochs, optimizer, criterion_mse, criterion_mae):
    loss_train_history, loss_val_history = [], []
    for epoch in range(num_epochs):
        epoch_train_loss_mse, epoch_train_loss_mae = 0, 0
        epoch_val_loss_mse, epoch_val_loss_mae = 0, 0
        model.train()
        for data in train_loader:
            img, _ = data
            img = img.to(device)
            output = model(img)
            loss_mse = criterion_mse(output, img)
            loss_mae = criterion_mae(output, img)
            optimizer.zero_grad()
            loss_mse.backward()
            optimizer.step()
            epoch_train_loss_mse += loss_mse.item()
            epoch_train_loss_mae += loss_mae.item()
        model.eval()
        for data in val_loader:
            img, _ = data
            img = img.to(device)
            with torch.no_grad():
                output = model(img)
                loss_mse = criterion_mse(output, img)
                loss_mae = criterion_mae(output, img)
            epoch_val_loss_mse += loss_mse.item()
            epoch_val_loss_mae += loss_mae.item()
        epoch_train_loss_mse = epoch_train_loss_mse / len(train_loader)
        epoch_train_loss_mae = epoch_train_loss_mae / len(train_loader)

        epoch_val_loss_mse = epoch_val_loss_mse / len(val_loader)
        epoch_val_loss_mae = epoch_val_loss_mae / len(val_loader)
        loss_train_history.append((epoch_train_loss_mse, epoch_train_loss_mae))
        loss_val_history.append((epoch_val_loss_mse, epoch_val_loss_mae))
        print(f'epoch [{epoch+1}/{num_epochs}], train MSE:{epoch_train_loss_mse:.4f}, train MAE:{epoch_train_loss_mae:.4f}, val MSE:{epoch_val_loss_mse:.4f}, val MAE:{epoch_val_loss_mae:.4f}')
    return loss_train_history, loss_val_history

def plot_loss(loss_train_history, loss_val_history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    train_mse, train_mae = list(zip(*loss_train_history))
    val_mse, val_mae = list(zip(*loss_val_history))

    ax1.plot(np.arange(len(train_mse)), train_mse, c='b', label='train MSE')
    ax1.plot(np.arange(len(val_mse)), val_mse, c='r', label='val MSE')
    ax1.legend()
    ax1.set_title('MSE Loss')

    ax2.plot(np.arange(len(train_mae)), train_mae, c='g', label='train MAE')
    ax2.plot(np.arange(len(val_mae)), val_mae, c='y', label='val MAE')
    ax2.legend()
    ax2.set_title('MAE Loss')

    plt.show()

model_mnist = ConvAutoencoder()
model_mnist.to(device)

optimizer_mnist = torch.optim.Adam(model_mnist.parameters(), lr=learning_rate, weight_decay=1e-6)
#optimizer_mnist = torch.optim.Adam(model_mnist.parameters(), lr=learning_rate)

criterion_mse = nn.MSELoss()
#criterion_mae = nn.L1Loss(reduction='mean')
criterion_mae = nn.L1Loss()

loss_train_history, loss_val_history = train_eval_model(model_mnist, device, train_mnist_loader, val_mnist_loader, num_epochs, optimizer_mnist, criterion_mse, criterion_mae)

plot_loss(loss_train_history, loss_val_history)

model_fashionmnist = ConvAutoencoder()
model_fashionmnist.to(device)

optimizer_fashionmnist = Adam(model_fashionmnist.parameters(), lr=learning_rate, weight_decay=1e-5)
criterion_mse = nn.MSELoss()
criterion_mae = nn.L1Loss()

loss_train_history_fashion, loss_val_history_fashion = train_eval_model(model_fashionmnist,device,train_fashionmnist_loader, val_fashionmnist_loader, num_epochs, optimizer_fashionmnist, criterion_mse, criterion_mae)

plot_loss(loss_train_history_fashion, loss_val_history_fashion)

def test_model(model, device, test_loader, criterion_mse, criterion_mae):
    model.eval()
    test_loss_mse, test_loss_mae = 0, 0
    with torch.no_grad():
        for data in test_loader:
            img, _ = data
            img = img.to(device)
            output = model(img)
            loss_mse = criterion_mse(output, img)
            loss_mae = criterion_mae(output, img)

            test_loss_mse += loss_mse.item()
            test_loss_mae += loss_mae.item()

    test_loss_mse = test_loss_mse / len(test_loader)
    test_loss_mae = test_loss_mae / len(test_loader)

    return test_loss_mse, test_loss_mae

test_loss_mse_mnist, test_loss_mae_mnist = test_model(model_mnist, device, test_mnist_loader, criterion_mse, criterion_mae)

print(f'MNIST Test MSE: {test_loss_mse_mnist:.4f}, MNIST Test MAE: {test_loss_mae_mnist:.4f}')

test_loss_mse_fashionmnist, test_loss_mae_fashionmnist = test_model(model_fashionmnist, device, test_fashionmnist_loader, criterion_mse, criterion_mae)

print(f'Fashion-MNIST Test MSE: {test_loss_mse_fashionmnist:.4f}, Fashion-MNIST Test MAE: {test_loss_mae_fashionmnist:.4f}')

def plot_samples(model, test_loader, device):
    test_iter = iter(test_loader)
    test_images, _ = next(test_iter)
    test_images = test_images.to(device)

    with torch.no_grad():
        rec_test_images = model(test_images)

    test_images = test_images.cpu().numpy()
    rec_test_images = rec_test_images.cpu().numpy()

    fig, axs = plt.subplots(nrows=2, ncols=7, figsize=(16, 4))
    for j in range(7):
        m = randrange(test_images.shape[0])
        axs[0, j].imshow(test_images[m, 0], cmap='gray')
        axs[1, j].imshow(rec_test_images[m, 0], cmap='gray')

    plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])
    plt.show()

plot_samples(model_mnist, test_mnist_loader, device)

plot_samples(model_mnist, test_mnist_loader, device)

# Test mnist dataset
plot_samples(model_mnist, test_mnist_loader, device)

plot_samples(model_mnist, val_mnist_loader, device)

# Valid mnist dataset
plot_samples(model_mnist, val_mnist_loader, device)

plot_samples(model_fashionmnist, test_fashionmnist_loader, device)

plot_samples(model_fashionmnist, test_fashionmnist_loader, device)

plot_samples(model_fashionmnist, test_fashionmnist_loader, device)

plot_samples(model_fashionmnist, val_fashionmnist_loader, device)

def extract_features_labels(loader, encoder):
    features = []
    labels = []
    for data in loader:
        img, label = data
        with torch.no_grad():
            feature = encoder(img).view(img.size(0), -1).cpu().numpy()
        features.append(feature)
        labels.append(label)
    features = np.vstack(features)
    labels = np.hstack(labels)
    return features, labels

train_mnist_features, train_mnist_labels = extract_features_labels(train_mnist_loader, model_mnist.encoder)
val_mnist_features, val_mnist_labels = extract_features_labels(val_mnist_loader, model_mnist.encoder)
test_mnist_features, test_mnist_labels = extract_features_labels(test_mnist_loader, model_mnist.encoder)

all_mnist_features = np.vstack([train_mnist_features, val_mnist_features, test_mnist_features])
all_mnist_labels = np.hstack([train_mnist_labels, val_mnist_labels, test_mnist_labels])

print("Shape of concatenated features for MNIST datasets:", all_mnist_features.shape)
print("Shape of concatenated labels for MNIST datasets:", all_mnist_labels.shape)

train_fashionmnist_features, train_fashionmnist_labels = extract_features_labels(train_fashionmnist_loader, model_fashionmnist.encoder)
val_fashionmnist_features, val_fashionmnist_labels = extract_features_labels(val_fashionmnist_loader, model_fashionmnist.encoder)
test_fashionmnist_features, test_fashionmnist_labels = extract_features_labels(test_fashionmnist_loader, model_fashionmnist.encoder)

all_fashionmnist_features = np.vstack([train_fashionmnist_features, val_fashionmnist_features, test_fashionmnist_features])
all_fashionmnist_labels = np.hstack([train_fashionmnist_labels, val_fashionmnist_labels, test_fashionmnist_labels])

print("Shape of concatenated features for MNIST datasets:", all_fashionmnist_features.shape)
print("Shape of concatenated labels for MNIST datasets:", all_fashionmnist_labels.shape)

def find_best_clusters(features, dataset_name):
    silhouette_scores = []
    for n_clusters in range(5, 16):
        kmeans = KMeans(n_clusters=n_clusters, random_state=0)
        cluster_labels = kmeans.fit_predict(features)
        silhouette_avg = silhouette_score(features, cluster_labels)
        silhouette_scores.append(silhouette_avg)

    best_n_clusters = np.argmax(silhouette_scores) + 5
    print(f"Best number of clusters for {dataset_name} is {best_n_clusters} based on silhouette score.")

    plt.figure(figsize=(10, 6))
    plt.plot(range(5, 16), silhouette_scores, marker='o')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Silhouette Score')
    plt.title(f'Silhouette Score vs Number of Clusters for {dataset_name}')
    plt.grid(True)
    plt.show()
    return best_n_clusters

# Finding best number of clusters for MNIST dataset
best_clusters_mnist = find_best_clusters(all_mnist_features, 'MNIST')

best_kmeans_mnist = KMeans(n_clusters=best_clusters_mnist, random_state=0)
cluster_labels_mnist = best_kmeans_mnist.fit_predict(all_mnist_features)

cluster_labels_count_mnist = []
for cluster_idx in range(10):
    cluster_labels = all_mnist_labels[cluster_labels_mnist == cluster_idx]
    cluster_labels_count = np.bincount(cluster_labels, minlength=10)
    cluster_labels_count_mnist.append(cluster_labels_count)

fig, ax = plt.subplots(figsize=(12, 6))

x = np.arange(10)
width = 0.8 / best_clusters_mnist
for i, cluster_label in enumerate(cluster_labels_count_mnist):
    ax.bar(x + i * width, cluster_label, width=width, label=f"Cluster {i+1}")

ax.set_xticks(x + (best_clusters_mnist - 1) * width / 2)
ax.set_xticklabels(range(10))

ax.set_ylabel("Number of each label in different Clusters (MNIST)")
ax.set_title("Number of each label in different Clusters for MNIST")

ax.legend()
plt.tight_layout()
plt.show()

best_clusters_fashionmnist = find_best_clusters(all_fashionmnist_features, 'Fashion-MNIST')

best_kmeans_fashionmnist = KMeans(n_clusters=best_clusters_fashionmnist, random_state=0)
cluster_labels_fashionmnist = best_kmeans_fashionmnist.fit_predict(all_fashionmnist_features)

cluster_labels_count_fashionmnist = []
for cluster_idx in range(10):
    cluster_labels = all_fashionmnist_labels[cluster_labels_fashionmnist == cluster_idx]
    cluster_labels_count = np.bincount(cluster_labels, minlength=10)
    cluster_labels_count_fashionmnist.append(cluster_labels_count)

fig, ax = plt.subplots(figsize=(12, 6))

x = np.arange(10)
width = 0.8 / best_clusters_fashionmnist
for i, cluster_label in enumerate(cluster_labels_count_fashionmnist):
    ax.bar(x + i * width, cluster_label, width=width, label=f"Cluster {i+1}")

ax.set_xticks(x + (best_clusters_fashionmnist - 1) * width / 2)
ax.set_xticklabels(range(10))

ax.set_ylabel("Number of each label in different Clusters (Fashion-MNIST)")
ax.set_title("Number of each label in different Clusters for Fashion-MNIST")

ax.legend()

plt.tight_layout()

plt.show()